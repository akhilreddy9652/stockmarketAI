#!/usr/bin/env python3
"""
Production-Ready Enhanced Projections System
Fixes current issues and dramatically improves forecast accuracy
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.regularizers import l2
import warnings
import joblib
import yfinance as yf
from datetime import datetime, timedelta
from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.model_selection import TimeSeriesSplit
import talib
from typing import Dict, List, Tuple, Optional
import os

# Suppress warnings and fix TensorFlow issues
warnings.filterwarnings('ignore')
tf.get_logger().setLevel('ERROR')
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

# Fix Keras compatibility issues
def mse_metric(y_true, y_pred):
    """Compatible MSE metric"""
    return tf.keras.metrics.mean_squared_error(y_true, y_pred)

def mae_metric(y_true, y_pred):
    """Compatible MAE metric"""
    return tf.keras.metrics.mean_absolute_error(y_true, y_pred)

class ProductionEnhancedProjections:
    """
    Production-Ready Enhanced Projections System
    Specifically designed to fix current performance issues
    """
    
    def __init__(self, symbol: str = "RELIANCE.NS"):
        self.symbol = symbol
        self.sequence_length = 30  # Reduced for better stability
        self.models = {}
        self.scalers = {}
        self.is_trained = False
        
        # Create directories
        os.makedirs('models', exist_ok=True)
        os.makedirs('results', exist_ok=True)
        
        print(f"ðŸš€ Initializing Production Enhanced Projections for {symbol}")
    
    def fetch_robust_data(self, start_date: str, end_date: str) -> pd.DataFrame:
        """
        Fetch data with robust error handling and validation
        """
        print(f"ðŸ“Š Fetching robust data for {self.symbol}...")
        
        try:
            ticker = yf.Ticker(self.symbol)
            df = ticker.history(start=start_date, end=end_date, interval='1d')
            
            if df.empty:
                # Try fallback symbols for Indian stocks
                if self.symbol.endswith('.NS'):
                    fallback_symbol = self.symbol.replace('.NS', '.BO')
                    print(f"âš ï¸ Trying fallback symbol: {fallback_symbol}")
                    ticker = yf.Ticker(fallback_symbol)
                    df = ticker.history(start=start_date, end=end_date, interval='1d')
                
                if df.empty:
                    raise ValueError(f"No data found for {self.symbol}")
            
            # Clean and validate data
            df.reset_index(inplace=True)
            df = df.dropna()
            
            # Remove invalid data
            df = df[df['Close'] > 0]
            df = df[df['Volume'] > 0]
            
            # Remove outliers (prices that change more than 50% in a day)
            price_change = df['Close'].pct_change().abs()
            df = df[price_change < 0.5]
            
            print(f"âœ… Fetched {len(df)} clean records")
            return df
            
        except Exception as e:
            print(f"âŒ Error fetching data: {str(e)}")
            raise
    
    def create_production_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Create production-grade features with error handling
        """
        print("ðŸ”§ Creating production features...")
        
        featured_df = df.copy()
        
        try:
            # Basic price features
            featured_df['Returns'] = featured_df['Close'].pct_change()
            featured_df['Log_Returns'] = np.log(featured_df['Close'] / featured_df['Close'].shift(1))
            
            # Volatility features
            for window in [5, 10, 20]:
                featured_df[f'Volatility_{window}'] = featured_df['Returns'].rolling(window).std()
                featured_df[f'Price_MA_{window}'] = featured_df['Close'].rolling(window).mean()
                featured_df[f'Price_to_MA_{window}'] = featured_df['Close'] / featured_df[f'Price_MA_{window}']
            
            # Volume features
            featured_df['Volume_MA_20'] = featured_df['Volume'].rolling(20).mean()
            featured_df['Volume_Ratio'] = featured_df['Volume'] / featured_df['Volume_MA_20']
            
            # Technical indicators with error handling
            try:
                # Core technical indicators
                featured_df['RSI_14'] = talib.RSI(featured_df['Close'].values, 14)
                featured_df['MACD'], featured_df['MACD_Signal'], _ = talib.MACD(featured_df['Close'].values)
                featured_df['BB_Upper'], featured_df['BB_Middle'], featured_df['BB_Lower'] = talib.BBANDS(featured_df['Close'].values)
                featured_df['ATR_14'] = talib.ATR(featured_df['High'].values, featured_df['Low'].values, featured_df['Close'].values, 14)
                
                # Bollinger Band features
                featured_df['BB_Width'] = (featured_df['BB_Upper'] - featured_df['BB_Lower']) / featured_df['BB_Middle']
                featured_df['BB_Position'] = (featured_df['Close'] - featured_df['BB_Lower']) / (featured_df['BB_Upper'] - featured_df['BB_Lower'])
                
                # Momentum features
                featured_df['Stoch_K'], featured_df['Stoch_D'] = talib.STOCH(
                    featured_df['High'].values, featured_df['Low'].values, featured_df['Close'].values
                )
                featured_df['Williams_R'] = talib.WILLR(
                    featured_df['High'].values, featured_df['Low'].values, featured_df['Close'].values
                )
                
            except Exception as e:
                print(f"âš ï¸ Warning: Some technical indicators failed: {str(e)}")
            
            # Price action features
            featured_df['HL_Ratio'] = featured_df['High'] / featured_df['Low']
            featured_df['OC_Ratio'] = featured_df['Open'] / featured_df['Close']
            featured_df['Price_Range'] = (featured_df['High'] - featured_df['Low']) / featured_df['Close']
            
            # Lag features
            for lag in [1, 2, 3, 5]:
                featured_df[f'Close_Lag_{lag}'] = featured_df['Close'].shift(lag)
                featured_df[f'Returns_Lag_{lag}'] = featured_df['Returns'].shift(lag)
            
            # Rolling statistics
            for window in [5, 10]:
                featured_df[f'Close_Std_{window}'] = featured_df['Close'].rolling(window).std()
                featured_df[f'Returns_Mean_{window}'] = featured_df['Returns'].rolling(window).mean()
            
            # Calendar features
            featured_df['DayOfWeek'] = featured_df['Date'].dt.dayofweek
            featured_df['Month'] = featured_df['Date'].dt.month
            featured_df['Quarter'] = featured_df['Date'].dt.quarter
            
            # Clean data
            featured_df = featured_df.dropna()
            featured_df = featured_df.replace([np.inf, -np.inf], np.nan).dropna()
            
            print(f"âœ… Created {len(featured_df.columns)} features")
            return featured_df
            
        except Exception as e:
            print(f"âŒ Error creating features: {str(e)}")
            raise
    
    def create_production_lstm_model(self, input_shape: Tuple[int, int]) -> Model:
        """
        Create production-ready LSTM model with proper error handling
        """
        model = Sequential([
            LSTM(64, return_sequences=True, input_shape=input_shape, 
                 kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01)),
            Dropout(0.2),
            BatchNormalization(),
            
            LSTM(32, return_sequences=False, 
                 kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01)),
            Dropout(0.2),
            BatchNormalization(),
            
            Dense(16, activation='relu', kernel_regularizer=l2(0.01)),
            Dropout(0.1),
            Dense(1, activation='linear')
        ])
        
        # Use Huber loss for robustness
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='huber',
            metrics=[mae_metric]
        )
        
        return model
    
    def create_ensemble_models(self, X_train: np.ndarray, y_train: np.ndarray, 
                              X_val: np.ndarray, y_val: np.ndarray) -> Dict:
        """
        Create and train ensemble of models
        """
        print("ðŸ—ï¸ Creating and training ensemble models...")
        
        models = {}
        
        # 1. Production LSTM
        lstm_model = self.create_production_lstm_model((X_train.shape[1], X_train.shape[2]))
        
        callbacks = [
            EarlyStopping(patience=15, restore_best_weights=True, monitor='val_loss'),
            ReduceLROnPlateau(factor=0.5, patience=8, min_lr=1e-6, monitor='val_loss')
        ]
        
        print("ðŸ‹ï¸ Training LSTM model...")
        lstm_history = lstm_model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=100,
            batch_size=32,
            callbacks=callbacks,
            verbose=0
        )
        
        models['lstm'] = {
            'model': lstm_model,
            'history': lstm_history.history,
            'type': 'deep_learning'
        }
        
        # 2. Random Forest (for comparison and ensemble diversity)
        print("ðŸŒ² Training Random Forest...")
        X_train_flat = X_train.reshape(X_train.shape[0], -1)
        X_val_flat = X_val.reshape(X_val.shape[0], -1)
        
        rf_model = RandomForestRegressor(
            n_estimators=100,
            max_depth=15,
            random_state=42,
            n_jobs=-1
        )
        rf_model.fit(X_train_flat, y_train)
        
        models['random_forest'] = {
            'model': rf_model,
            'type': 'traditional'
        }
        
        # 3. Gradient Boosting
        print("ðŸš€ Training Gradient Boosting...")
        gb_model = GradientBoostingRegressor(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=42
        )
        gb_model.fit(X_train_flat, y_train)
        
        models['gradient_boosting'] = {
            'model': gb_model,
            'type': 'traditional'
        }
        
        print("âœ… Ensemble models created and trained")
        return models
    
    def prepare_data_sequences(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """
        Prepare data sequences for training
        """
        print("ðŸ“Š Preparing data sequences...")
        
        # Select features
        exclude_cols = ['Date', 'Close']
        feature_cols = [col for col in df.columns 
                       if col not in exclude_cols and df[col].dtype in [np.float64, np.int64]]
        
        # Prepare features and target
        features = df[feature_cols].values
        target = df['Close'].values
        
        # Create sequences
        X, y = [], []
        for i in range(self.sequence_length, len(features)):
            X.append(features[i-self.sequence_length:i])
            y.append(target[i])
        
        X = np.array(X)
        y = np.array(y)
        
        print(f"âœ… Created {len(X)} sequences with {X.shape[2]} features")
        return X, y, feature_cols
    
    def train_production_system(self, symbol: str = None) -> Dict:
        """
        Train the complete production system
        """
        if symbol:
            self.symbol = symbol
            
        print(f"ðŸš€ Training production system for {self.symbol}")
        
        # Fetch data
        end_date = datetime.now()
        start_date = end_date - timedelta(days=730)  # 2 years of data
        
        df = self.fetch_robust_data(start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'))
        
        # Create features
        featured_df = self.create_production_features(df)
        
        # Prepare sequences
        X, y, feature_cols = self.prepare_data_sequences(featured_df)
        
        # Time series split
        split_idx = int(len(X) * 0.8)
        X_train, X_val = X[:split_idx], X[split_idx:]
        y_train, y_val = y[:split_idx], y[split_idx:]
        
        # Scale features using RobustScaler (better for outliers)
        feature_scaler = RobustScaler()
        X_train_scaled = feature_scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)
        X_val_scaled = feature_scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)
        
        # Scale target
        target_scaler = RobustScaler()
        y_train_scaled = target_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()
        y_val_scaled = target_scaler.transform(y_val.reshape(-1, 1)).flatten()
        
        # Save scalers
        self.scalers = {
            'feature_scaler': feature_scaler,
            'target_scaler': target_scaler,
            'feature_cols': feature_cols
        }
        
        # Train ensemble
        self.models = self.create_ensemble_models(X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled)
        
        # Evaluate models
        results = {}
        for name, model_data in self.models.items():
            model = model_data['model']
            
            if model_data['type'] == 'deep_learning':
                val_predictions = model.predict(X_val_scaled, verbose=0).flatten()
            else:
                val_predictions = model.predict(X_val_scaled.reshape(X_val_scaled.shape[0], -1))
            
            # Inverse transform predictions
            val_predictions = target_scaler.inverse_transform(val_predictions.reshape(-1, 1)).flatten()
            y_val_original = target_scaler.inverse_transform(y_val_scaled.reshape(-1, 1)).flatten()
            
            # Calculate metrics
            mse = mean_squared_error(y_val_original, val_predictions)
            mae = mean_absolute_error(y_val_original, val_predictions)
            rmse = np.sqrt(mse)
            mape = np.mean(np.abs((y_val_original - val_predictions) / y_val_original)) * 100
            
            # Directional accuracy
            actual_direction = np.diff(y_val_original) > 0
            pred_direction = np.diff(val_predictions) > 0
            directional_accuracy = np.mean(actual_direction == pred_direction) * 100
            
            # R-squared
            ss_res = np.sum((y_val_original - val_predictions) ** 2)
            ss_tot = np.sum((y_val_original - np.mean(y_val_original)) ** 2)
            r2 = 1 - (ss_res / ss_tot)
            
            results[name] = {
                'mse': mse,
                'mae': mae,
                'rmse': rmse,
                'mape': mape,
                'directional_accuracy': directional_accuracy,
                'r2': r2,
                'predictions': val_predictions,
                'actual': y_val_original
            }
            
            print(f"âœ… {name}:")
            print(f"   RMSE: ${rmse:.2f}")
            print(f"   MAPE: {mape:.2f}%")
            print(f"   Directional Accuracy: {directional_accuracy:.1f}%")
            print(f"   RÂ²: {r2:.3f}")
        
        # Save everything
        self.save_production_system()
        self.is_trained = True
        
        print("âœ… Production system training completed!")
        return results
    
    def generate_enhanced_forecast(self, forecast_days: int = 30) -> Dict:
        """
        Generate enhanced forecast with confidence intervals
        """
        if not self.is_trained:
            print("âš ï¸ System not trained. Training now...")
            self.train_production_system()
        
        print(f"ðŸ”® Generating enhanced {forecast_days}-day forecast...")
        
        # Fetch latest data
        end_date = datetime.now()
        start_date = end_date - timedelta(days=365)
        
        df = self.fetch_robust_data(start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'))
        featured_df = self.create_production_features(df)
        
        # Prepare last sequence
        features = featured_df[self.scalers['feature_cols']].values
        last_sequence = features[-self.sequence_length:]
        
        # Scale features
        feature_scaler = self.scalers['feature_scaler']
        target_scaler = self.scalers['target_scaler']
        
        last_sequence_scaled = feature_scaler.transform(
            last_sequence.reshape(-1, features.shape[-1])
        ).reshape(1, self.sequence_length, -1)
        
        # Generate predictions from all models
        predictions = {}
        ensemble_predictions = []
        
        for name, model_data in self.models.items():
            model = model_data['model']
            
            if model_data['type'] == 'deep_learning':
                pred = model.predict(last_sequence_scaled, verbose=0)[0, 0]
            else:
                pred = model.predict(last_sequence_scaled.reshape(1, -1))[0]
            
            # Inverse transform
            pred = target_scaler.inverse_transform([[pred]])[0, 0]
            predictions[name] = pred
            ensemble_predictions.append(pred)
        
        # Calculate ensemble statistics
        ensemble_mean = np.mean(ensemble_predictions)
        ensemble_std = np.std(ensemble_predictions)
        ensemble_median = np.median(ensemble_predictions)
        
        # Confidence intervals
        confidence_95_lower = ensemble_mean - 1.96 * ensemble_std
        confidence_95_upper = ensemble_mean + 1.96 * ensemble_std
        
        # Current price
        current_price = df['Close'].iloc[-1]
        
        # Price change analysis
        price_change = ensemble_mean - current_price
        price_change_pct = (price_change / current_price) * 100
        
        # Generate recommendation
        if price_change_pct > 5:
            recommendation = "ðŸŸ¢ Strong Buy"
            confidence = "High" if ensemble_std / current_price < 0.05 else "Medium"
        elif price_change_pct > 2:
            recommendation = "ðŸŸ¢ Buy"
            confidence = "Medium"
        elif price_change_pct > -2:
            recommendation = "ðŸŸ¡ Hold"
            confidence = "Medium"
        elif price_change_pct > -5:
            recommendation = "ðŸ”´ Sell"
            confidence = "Medium"
        else:
            recommendation = "ðŸ”´ Strong Sell"
            confidence = "High" if ensemble_std / current_price < 0.05 else "Medium"
        
        results = {
            'symbol': self.symbol,
            'forecast_date': datetime.now().strftime('%Y-%m-%d'),
            'current_price': current_price,
            'individual_predictions': predictions,
            'ensemble_prediction': ensemble_mean,
            'ensemble_median': ensemble_median,
            'ensemble_std': ensemble_std,
            'confidence_interval_95': {
                'lower': confidence_95_lower,
                'upper': confidence_95_upper
            },
            'price_change': price_change,
            'price_change_pct': price_change_pct,
            'recommendation': recommendation,
            'confidence': confidence,
            'forecast_horizon_days': forecast_days,
            'model_agreement': 'High' if ensemble_std / current_price < 0.02 else 'Medium' if ensemble_std / current_price < 0.05 else 'Low'
        }
        
        print(f"âœ… Enhanced forecast generated:")
        print(f"   Current Price: ${current_price:.2f}")
        print(f"   Predicted Price: ${ensemble_mean:.2f}")
        print(f"   Expected Change: {price_change_pct:+.2f}%")
        print(f"   Recommendation: {recommendation}")
        print(f"   Confidence: {confidence}")
        
        return results
    
    def save_production_system(self):
        """
        Save the trained production system
        """
        print("ðŸ’¾ Saving production system...")
        
        # Save scalers
        joblib.dump(self.scalers, f'models/{self.symbol}_production_scalers.pkl')
        
        # Save deep learning models
        for name, model_data in self.models.items():
            if model_data['type'] == 'deep_learning':
                model_data['model'].save(f'models/{self.symbol}_{name}_production.keras')
            else:
                joblib.dump(model_data['model'], f'models/{self.symbol}_{name}_production.pkl')
        
        print("âœ… Production system saved")
    
    def load_production_system(self):
        """
        Load the trained production system
        """
        print("ðŸ“‚ Loading production system...")
        
        try:
            # Load scalers
            self.scalers = joblib.load(f'models/{self.symbol}_production_scalers.pkl')
            
            # Load models
            self.models = {}
            
            # Load LSTM model
            if os.path.exists(f'models/{self.symbol}_lstm_production.keras'):
                lstm_model = tf.keras.models.load_model(f'models/{self.symbol}_lstm_production.keras')
                self.models['lstm'] = {'model': lstm_model, 'type': 'deep_learning'}
            
            # Load traditional models
            for model_name in ['random_forest', 'gradient_boosting']:
                if os.path.exists(f'models/{self.symbol}_{model_name}_production.pkl'):
                    model = joblib.load(f'models/{self.symbol}_{model_name}_production.pkl')
                    self.models[model_name] = {'model': model, 'type': 'traditional'}
            
            self.is_trained = True
            print("âœ… Production system loaded")
            
        except Exception as e:
            print(f"âš ï¸ Could not load production system: {str(e)}")
            print("Training new system...")
            self.train_production_system()
    
    def run_production_analysis(self, symbol: str = None, forecast_days: int = 30) -> Dict:
        """
        Run complete production analysis
        """
        if symbol:
            self.symbol = symbol
        
        print(f"ðŸš€ Running production analysis for {self.symbol}")
        
        # Try to load existing system, otherwise train new one
        self.load_production_system()
        
        # Generate forecast
        forecast_results = self.generate_enhanced_forecast(forecast_days)
        
        # Save results
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        joblib.dump(forecast_results, f'results/{self.symbol}_production_forecast_{timestamp}.pkl')
        
        print("âœ… Production analysis completed!")
        return forecast_results

if __name__ == "__main__":
    # Example usage for Indian stock
    projector = ProductionEnhancedProjections("RELIANCE.NS")
    results = projector.run_production_analysis(forecast_days=30)
    print(f"\nProduction Analysis Results:")
    print(f"Symbol: {results['symbol']}")
    print(f"Current Price: â‚¹{results['current_price']:.2f}")
    print(f"Predicted Price: â‚¹{results['ensemble_prediction']:.2f}")
    print(f"Expected Change: {results['price_change_pct']:+.2f}%")
    print(f"Recommendation: {results['recommendation']}") 